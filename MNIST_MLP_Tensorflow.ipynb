{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 28*28    #feature space dimension \n",
    "x_train = x_train.reshape((60000, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(x_train)\n",
    "N_test = len(x_test)\n",
    "\n",
    "x_test = x_test.reshape((N_test, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_train/225\n",
    "x_test_scaled = x_test/225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "neurons_hid1 = 512\n",
    "neurons_hid2 = 512\n",
    "num_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actf1 = tf.nn.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actf2 = tf.nn.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(initializer([num_inputs, neurons_hid1]), dtype=tf.float32)\n",
    "w2 = tf.Variable(initializer([neurons_hid1, neurons_hid2]), dtype=tf.float32)\n",
    "w3 = tf.Variable(initializer([neurons_hid2, num_outputs]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = tf.Variable(tf.zeros(neurons_hid1))\n",
    "b2 = tf.Variable(tf.zeros(neurons_hid2))\n",
    "b3 = tf.Variable(tf.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_layer1 = actf1(tf.matmul(X,w1)+b1)\n",
    "hid_layer2 = actf1(tf.matmul(hid_layer1,w2)+b2)\n",
    "output_layer = tf.matmul(hid_layer2,w3)+b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.uint8, shape=[None, 10])\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train_onehot = pd.get_dummies(y_train.astype(str))\n",
    "y_test_onehot = pd.get_dummies(y_test.astype(str))\n",
    "y_train_onehot = y_train_onehot.values\n",
    "y_test_onehot = y_test_onehot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 Loss: 0.013852857053279877\n",
      "epoch: 1 Loss: 0.08621084690093994\n",
      "epoch: 2 Loss: 0.005981272552162409\n",
      "epoch: 3 Loss: 0.002842166693881154\n",
      "epoch: 4 Loss: 0.014916513115167618\n",
      "epoch: 5 Loss: 0.027732301503419876\n",
      "epoch: 6 Loss: 0.002706584520637989\n",
      "epoch: 7 Loss: 0.42140066623687744\n",
      "epoch: 8 Loss: 0.005839128512889147\n",
      "epoch: 9 Loss: 0.008478319272398949\n",
      "epoch: 10 Loss: 0.0021829463075846434\n",
      "epoch: 11 Loss: 0.004434795118868351\n",
      "epoch: 12 Loss: 0.23993350565433502\n",
      "epoch: 13 Loss: 0.10614810138940811\n",
      "epoch: 14 Loss: 0.0725460797548294\n",
      "epoch: 15 Loss: 0.00011744648509193212\n",
      "epoch: 16 Loss: 0.0005014874041080475\n",
      "epoch: 17 Loss: 0.0005604631151072681\n",
      "epoch: 18 Loss: 0.00015316830831579864\n",
      "epoch: 19 Loss: 0.001013266621157527\n",
      "epoch: 20 Loss: 0.00031889573438093066\n",
      "epoch: 21 Loss: 0.0014722872292622924\n",
      "epoch: 22 Loss: 5.250057438388467e-05\n",
      "epoch: 23 Loss: 7.4505797087454084e-09\n",
      "epoch: 24 Loss: 3.5575042147684144e-06\n",
      "epoch: 25 Loss: 2.831207837061811e-07\n",
      "epoch: 26 Loss: 0.06725523620843887\n",
      "epoch: 27 Loss: 0.06130312383174896\n",
      "epoch: 28 Loss: 3.038558133994229e-05\n",
      "epoch: 29 Loss: 1.9726319806068204e-05\n",
      "0.9462\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        num_batches = len(x_train_scaled)//batch_size\n",
    "        x_batches = np.array_split(x_train_scaled, num_batches)\n",
    "        y_batches = np.array_split(y_train_onehot, num_batches)\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            \n",
    "            X_batch = x_batches[i]\n",
    "            Y_batch = y_batches[i]\n",
    "            \n",
    "            sess.run(train,feed_dict={X:X_batch, Y:Y_batch})\n",
    "            \n",
    "        training_loss = loss.eval(feed_dict={X:X_batch, Y:Y_batch})\n",
    "        \n",
    "        print('epoch: {} Loss: {}'.format(epoch,training_loss))\n",
    "        \n",
    "    # Evaluating\n",
    "    correct_prediction = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y,1))\n",
    "    \n",
    "    # [True, False, True, ...] --> [1, 0, 1, ...]\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(sess.run(acc, feed_dict={X: x_test_scaled, Y: y_test_onehot}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
